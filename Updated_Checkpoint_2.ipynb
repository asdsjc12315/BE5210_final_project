{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV3j0qyB4Eqx",
        "outputId": "1793da63-ce70-4292-afeb-3af277144046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 1 – Ridge CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:26<00:00, 223.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → fold 1: best α = 5.736e+03\n",
            "    → fold 2: best α = 5.736e+03\n",
            "    → fold 3: best α = 5.736e+03\n",
            "    → fold 4: best α = 5.736e+03\n",
            "    → fold 5: best α = 5.736e+03\n",
            " → Avg Pearson r (Ridge): 0.607\n",
            "\n",
            "=== Subject 2 – Ridge CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:23<00:00, 257.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → fold 1: best α = 4.520e+03\n",
            "    → fold 2: best α = 4.520e+03\n",
            "    → fold 3: best α = 4.520e+03\n",
            "    → fold 4: best α = 4.520e+03\n",
            "    → fold 5: best α = 4.520e+03\n",
            " → Avg Pearson r (Ridge): 0.497\n",
            "\n",
            "=== Subject 3 – Ridge CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:26<00:00, 225.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → fold 1: best α = 3.562e+03\n",
            "    → fold 2: best α = 3.562e+03\n",
            "    → fold 3: best α = 3.562e+03\n",
            "    → fold 4: best α = 3.562e+03\n",
            "    → fold 5: best α = 3.562e+03\n",
            " → Avg Pearson r (Ridge): 0.703\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from scipy import signal\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# ========== Parameters ==========\n",
        "fs = 1000\n",
        "window_len = 100\n",
        "overlap = 50\n",
        "step = window_len - overlap\n",
        "N_wind = 3\n",
        "n_splits = 5\n",
        "\n",
        "# ========== Gaussian Smoothing (All points) ==========\n",
        "def gaussian_smoothing(pred, sigma=2.25):\n",
        "    pred_smooth = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        pred_smooth[:, i] = gaussian_filter1d(pred[:, i], sigma=sigma)\n",
        "    return pred_smooth\n",
        "\n",
        "# ========== Outlier Suppression (Set to 0) ==========\n",
        "def suppress_low_outliers(pred, threshold_multiplier=2):\n",
        "    pred_cleaned = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        col = pred[:, i]\n",
        "        mean = np.mean(col)\n",
        "        std = np.std(col)\n",
        "        threshold = mean - threshold_multiplier * std\n",
        "        pred_cleaned[:, i] = np.where(col < threshold, 0, col)\n",
        "    return pred_cleaned\n",
        "\n",
        "# ========== Bandpass Filter ==========\n",
        "def bandpass_filter(data, fs, lowcut, highcut, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "# ========== Feature Extraction ==========\n",
        "def get_features(window, fs=1000):\n",
        "    freq_bands = [(5, 15), (20, 25), (75, 115), (125, 160), (160, 175)]\n",
        "    n_channels = window.shape[1]\n",
        "    features = np.zeros((n_channels, 6))\n",
        "    features[:, 0] = np.mean(window, axis=0)\n",
        "    for i, (low, high) in enumerate(freq_bands):\n",
        "        band_filtered = bandpass_filter(window, fs, low, high)\n",
        "        features[:, i + 1] = np.mean(np.abs(band_filtered), axis=0)\n",
        "    return features\n",
        "\n",
        "# ========== Sliding Window ==========\n",
        "def get_windowed_feats(ecog, fs, win_len, overlap):\n",
        "    step = win_len - overlap\n",
        "    num_windows = (ecog.shape[0] - overlap) // step\n",
        "    feats = []\n",
        "    for i in tqdm(range(num_windows), desc=\"Extracting features\"):\n",
        "        start = i * step\n",
        "        end = start + win_len\n",
        "        if end > ecog.shape[0]:\n",
        "            break\n",
        "        window = ecog[start:end, :]\n",
        "        feats.append(get_features(window, fs).flatten())\n",
        "    return np.array(feats)\n",
        "\n",
        "# ========== Create R Matrix ==========\n",
        "def create_R_matrix(features, N_wind):\n",
        "    num_windows, num_feats = features.shape\n",
        "    pad = np.tile(features[0], (N_wind - 1, 1))\n",
        "    padded = np.vstack([pad, features])\n",
        "    R = np.zeros((num_windows, 1 + N_wind * num_feats))\n",
        "    for i in range(num_windows):\n",
        "        context = padded[i:i + N_wind].flatten()\n",
        "        R[i] = np.concatenate(([1], context))\n",
        "    return R\n",
        "\n",
        "# ========== make scorer to maximise pearsonr ==========\n",
        "def mean_pearsonr(y_true, y_pred):\n",
        "    rs = [pearsonr(y_true[:,i], y_pred[:,i])[0]\n",
        "          for i in range(y_true.shape[1])]\n",
        "    return np.mean(rs)\n",
        "\n",
        "pearson_scorer = make_scorer(mean_pearsonr, greater_is_better=True)\n",
        "\n",
        "\n",
        "# ========== Cross-validation with Ridge ==========\n",
        "train_data   = scipy.io.loadmat('raw_training_data.mat')\n",
        "train_ecogs  = train_data['train_ecog']\n",
        "train_gloves = train_data['train_dg']\n",
        "\n",
        "\n",
        "\n",
        "for subj_idx in range(3):\n",
        "    print(f\"\\n=== Subject {subj_idx+1} – Ridge CV ===\")\n",
        "    ecog  = train_ecogs[subj_idx].item()\n",
        "    glove = train_gloves[subj_idx].item()\n",
        "\n",
        "    feats      = get_windowed_feats(ecog, fs, window_len, overlap)\n",
        "    R          = create_R_matrix(feats, N_wind)\n",
        "    glove_down = signal.decimate(glove,\n",
        "                                 glove.shape[0] // R.shape[0],\n",
        "                                 axis=0, zero_phase=True)[:R.shape[0]]\n",
        "    # smooth labels\n",
        "    for i in range(glove_down.shape[1]):\n",
        "        glove_down[:,i] = gaussian_filter1d(glove_down[:,i], sigma=2.25)\n",
        "\n",
        "    kf    = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    r_vals = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(R)):\n",
        "        # split\n",
        "        R_train, R_test = R[train_idx], R[test_idx]\n",
        "        y_train, y_test = glove_down[train_idx], glove_down[test_idx]\n",
        "\n",
        "        # scale features (skip intercept column at 0)\n",
        "        scaler = StandardScaler()\n",
        "        R_train[:,1:] = scaler.fit_transform(R_train[:,1:])\n",
        "        R_test [:,1:] = scaler.transform(   R_test[:,1:])\n",
        "\n",
        "        # ——— GridSearchCV to pick α that maximizes mean Pearson-r ———\n",
        "        alphas = np.logspace(2, 5, 30)\n",
        "        param_grid = {'alpha': alphas}\n",
        "        inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "        grid = GridSearchCV(\n",
        "            Ridge(),\n",
        "            param_grid,\n",
        "            scoring=pearson_scorer,\n",
        "            cv=inner_cv,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid.fit(R_train[:,1:], y_train)\n",
        "        best_alpha = grid.best_params_['alpha']\n",
        "        best_model = grid.best_estimator_\n",
        "\n",
        "        print(f\"    → fold {fold_idx+1}: best α = {best_alpha:.3e}\")\n",
        "\n",
        "        y_pred = best_model.predict(R_test[:,1:])\n",
        "\n",
        "        # post-process\n",
        "        y_pred = suppress_low_outliers(y_pred, threshold_multiplier=2)\n",
        "        y_pred = gaussian_smoothing(y_pred, sigma=2.25)\n",
        "\n",
        "        # evaluate\n",
        "        r = [pearsonr(y_test[:,i], y_pred[:,i])[0] for i in range(y_test.shape[1])]\n",
        "        r_vals.append(np.mean(r))\n",
        "\n",
        "        '''\n",
        "        # optional: plot for forst fold\n",
        "        if fold_idx == 0:\n",
        "            max_tp = min(int(60*fs/step), len(y_test))\n",
        "            time   = np.arange(max_tp)*step/fs\n",
        "            fig, axs = plt.subplots(5,1, figsize=(12,8), sharex=True)\n",
        "            fig.suptitle(f\"Subject {subj_idx+1} Fold 1\", fontsize=14)\n",
        "            for ch in range(5):\n",
        "                axs[ch].plot(time, y_test[:max_tp,ch], label='True')\n",
        "                axs[ch].plot(time, y_pred[:max_tp,ch], '--', label='Pred')\n",
        "                axs[ch].set_ylabel(f\"Finger {ch+1}\")\n",
        "                axs[ch].legend(loc='upper right')\n",
        "            axs[-1].set_xlabel(\"Time (s)\")\n",
        "            plt.tight_layout(rect=[0,0,1,0.95])\n",
        "            plt.show()\n",
        "        '''\n",
        "\n",
        "    print(f\" → Avg Pearson r (Ridge): {np.mean(r_vals):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tune alphas"
      ],
      "metadata": {
        "id": "LAqR0asqmwZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from scipy import signal\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# ========== Parameters ==========\n",
        "fs = 1000\n",
        "window_len = 100\n",
        "overlap = 50\n",
        "step = window_len - overlap\n",
        "N_wind = 3\n",
        "n_splits = 5\n",
        "coarse_alphas = [5.736e3, 4.520e3, 3.562e3]\n",
        "\n",
        "# ========== Gaussian Smoothing (All points) ==========\n",
        "def gaussian_smoothing(pred, sigma=2.25):\n",
        "    pred_smooth = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        pred_smooth[:, i] = gaussian_filter1d(pred[:, i], sigma=sigma)\n",
        "    return pred_smooth\n",
        "\n",
        "# ========== Outlier Suppression (Set to 0) ==========\n",
        "def suppress_low_outliers(pred, threshold_multiplier=2):\n",
        "    pred_cleaned = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        col = pred[:, i]\n",
        "        mean = np.mean(col)\n",
        "        std = np.std(col)\n",
        "        threshold = mean - threshold_multiplier * std\n",
        "        pred_cleaned[:, i] = np.where(col < threshold, 0, col)\n",
        "    return pred_cleaned\n",
        "\n",
        "# ========== Bandpass Filter ==========\n",
        "def bandpass_filter(data, fs, lowcut, highcut, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "# ========== Feature Extraction ==========\n",
        "def get_features(window, fs=1000):\n",
        "    freq_bands = [(5, 15), (20, 25), (75, 115), (125, 160), (160, 175)]\n",
        "    n_channels = window.shape[1]\n",
        "    features = np.zeros((n_channels, 6))\n",
        "    features[:, 0] = np.mean(window, axis=0)\n",
        "    for i, (low, high) in enumerate(freq_bands):\n",
        "        band_filtered = bandpass_filter(window, fs, low, high)\n",
        "        features[:, i + 1] = np.mean(np.abs(band_filtered), axis=0)\n",
        "    return features\n",
        "\n",
        "# ========== Sliding Window ==========\n",
        "def get_windowed_feats(ecog, fs, win_len, overlap):\n",
        "    step = win_len - overlap\n",
        "    num_windows = (ecog.shape[0] - overlap) // step\n",
        "    feats = []\n",
        "    for i in tqdm(range(num_windows), desc=\"Extracting features\"):\n",
        "        start = i * step\n",
        "        end = start + win_len\n",
        "        if end > ecog.shape[0]:\n",
        "            break\n",
        "        window = ecog[start:end, :]\n",
        "        feats.append(get_features(window, fs).flatten())\n",
        "    return np.array(feats)\n",
        "\n",
        "# ========== Create R Matrix ==========\n",
        "def create_R_matrix(features, N_wind):\n",
        "    num_windows, num_feats = features.shape\n",
        "    pad = np.tile(features[0], (N_wind - 1, 1))\n",
        "    padded = np.vstack([pad, features])\n",
        "    R = np.zeros((num_windows, 1 + N_wind * num_feats))\n",
        "    for i in range(num_windows):\n",
        "        context = padded[i:i + N_wind].flatten()\n",
        "        R[i] = np.concatenate(([1], context))\n",
        "    return R\n",
        "\n",
        "# ========== make scorer to maximise pearsonr ==========\n",
        "def mean_pearsonr(y_true, y_pred):\n",
        "    rs = [pearsonr(y_true[:,i], y_pred[:,i])[0]\n",
        "          for i in range(y_true.shape[1])]\n",
        "    return np.mean(rs)\n",
        "\n",
        "pearson_scorer = make_scorer(mean_pearsonr, greater_is_better=True)\n",
        "\n",
        "\n",
        "# ========== Cross-validation with Ridge ==========\n",
        "train_data   = scipy.io.loadmat('raw_training_data.mat')\n",
        "train_ecogs  = train_data['train_ecog']\n",
        "train_gloves = train_data['train_dg']\n",
        "\n",
        "\n",
        "\n",
        "for subj_idx in range(3):\n",
        "    print(f\"\\n=== Subject {subj_idx+1} – Ridge CV ===\")\n",
        "    ecog  = train_ecogs[subj_idx].item()\n",
        "    glove = train_gloves[subj_idx].item()\n",
        "\n",
        "    feats      = get_windowed_feats(ecog, fs, window_len, overlap)\n",
        "    R          = create_R_matrix(feats, N_wind)\n",
        "    glove_down = signal.decimate(glove,\n",
        "                                 glove.shape[0] // R.shape[0],\n",
        "                                 axis=0, zero_phase=True)[:R.shape[0]]\n",
        "    # smooth labels\n",
        "    for i in range(glove_down.shape[1]):\n",
        "        glove_down[:,i] = gaussian_filter1d(glove_down[:,i], sigma=2.25)\n",
        "\n",
        "    kf    = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    r_vals = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(R)):\n",
        "        # split\n",
        "        R_train, R_test = R[train_idx], R[test_idx]\n",
        "        y_train, y_test = glove_down[train_idx], glove_down[test_idx]\n",
        "\n",
        "        # scale features (skip intercept column at 0)\n",
        "        scaler = StandardScaler()\n",
        "        R_train[:,1:] = scaler.fit_transform(R_train[:,1:])\n",
        "        R_test [:,1:] = scaler.transform(   R_test[:,1:])\n",
        "\n",
        "        # ——— GridSearchCV to pick α that maximizes mean Pearson-r ———\n",
        "        # pick the coarse α for this subject\n",
        "        coarse_alpha = coarse_alphas[subj_idx]\n",
        "\n",
        "        # build a tight, high-res grid around it (±×3, 50 points)\n",
        "        lowalpha  = coarse_alpha / 3\n",
        "        highalpha = coarse_alpha * 3\n",
        "        alphas_refined = np.logspace(np.log10(lowalpha), np.log10(highalpha), 50)\n",
        "\n",
        "        param_grid = {'alpha': alphas_refined}\n",
        "        inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "        grid = GridSearchCV(\n",
        "            Ridge(),\n",
        "            param_grid,\n",
        "            scoring=pearson_scorer,\n",
        "            cv=inner_cv,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid.fit(R_train[:,1:], y_train)\n",
        "        best_alpha = grid.best_params_['alpha']\n",
        "        best_model = grid.best_estimator_\n",
        "\n",
        "        print(f\"    → fold {fold_idx+1}: best α = {best_alpha:.3e}\")\n",
        "\n",
        "        y_pred = best_model.predict(R_test[:,1:])\n",
        "\n",
        "        # post-process\n",
        "        y_pred = suppress_low_outliers(y_pred, threshold_multiplier=2)\n",
        "        y_pred = gaussian_smoothing(y_pred, sigma=2.25)\n",
        "\n",
        "        # evaluate\n",
        "        r = [pearsonr(y_test[:,i], y_pred[:,i])[0] for i in range(y_test.shape[1])]\n",
        "        r_vals.append(np.mean(r))\n",
        "\n",
        "        '''\n",
        "        # optional: plot for forst fold\n",
        "        if fold_idx == 0:\n",
        "            max_tp = min(int(60*fs/step), len(y_test))\n",
        "            time   = np.arange(max_tp)*step/fs\n",
        "            fig, axs = plt.subplots(5,1, figsize=(12,8), sharex=True)\n",
        "            fig.suptitle(f\"Subject {subj_idx+1} Fold 1\", fontsize=14)\n",
        "            for ch in range(5):\n",
        "                axs[ch].plot(time, y_test[:max_tp,ch], label='True')\n",
        "                axs[ch].plot(time, y_pred[:max_tp,ch], '--', label='Pred')\n",
        "                axs[ch].set_ylabel(f\"Finger {ch+1}\")\n",
        "                axs[ch].legend(loc='upper right')\n",
        "            axs[-1].set_xlabel(\"Time (s)\")\n",
        "            plt.tight_layout(rect=[0,0,1,0.95])\n",
        "            plt.show()\n",
        "        '''\n",
        "\n",
        "    print(f\" → Avg Pearson r (Ridge): {np.mean(r_vals):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yjbxpwDkaH5",
        "outputId": "e945479a-35e9-48aa-cd15-c6f389e1988a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 1 – Ridge CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:37<00:00, 161.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → fold 1: best α = 5.363e+03\n",
            "    → fold 2: best α = 5.609e+03\n",
            "    → fold 3: best α = 5.866e+03\n",
            "    → fold 4: best α = 5.609e+03\n",
            "    → fold 5: best α = 5.363e+03\n",
            " → Avg Pearson r (Ridge): 0.607\n",
            "\n",
            "=== Subject 2 – Ridge CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:28<00:00, 212.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → fold 1: best α = 5.056e+03\n",
            "    → fold 2: best α = 4.622e+03\n",
            "    → fold 3: best α = 4.622e+03\n",
            "    → fold 4: best α = 4.834e+03\n",
            "    → fold 5: best α = 5.056e+03\n",
            " → Avg Pearson r (Ridge): 0.497\n",
            "\n",
            "=== Subject 3 – Ridge CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:27<00:00, 215.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → fold 1: best α = 3.985e+03\n",
            "    → fold 2: best α = 3.810e+03\n",
            "    → fold 3: best α = 3.985e+03\n",
            "    → fold 4: best α = 3.810e+03\n",
            "    → fold 5: best α = 3.810e+03\n",
            " → Avg Pearson r (Ridge): 0.702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_alphas = [5.609e+03, 4.834e+03, 3.810e+03]"
      ],
      "metadata": {
        "id": "nXTaCggUng22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from scipy import signal\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "\n",
        "# ========== Parameters ==========\n",
        "fs = 1000\n",
        "window_len = 100\n",
        "overlap = 50\n",
        "step = window_len - overlap\n",
        "N_wind = 3\n",
        "n_splits = 5\n",
        "final_alphas = [5.609e3, 4.834e3, 3.810e3]\n",
        "\n",
        "# ========== Gaussian Smoothing (All points) ==========\n",
        "def gaussian_smoothing(pred, sigma=2.25):\n",
        "    pred_smooth = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        pred_smooth[:, i] = gaussian_filter1d(pred[:, i], sigma=sigma)\n",
        "    return pred_smooth\n",
        "\n",
        "# ========== Outlier Suppression (Set to 0) ==========\n",
        "def suppress_low_outliers(pred, threshold_multiplier=2):\n",
        "    pred_cleaned = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        col = pred[:, i]\n",
        "        mean = np.mean(col)\n",
        "        std = np.std(col)\n",
        "        threshold = mean - threshold_multiplier * std\n",
        "        pred_cleaned[:, i] = np.where(col < threshold, 0, col)\n",
        "    return pred_cleaned\n",
        "\n",
        "# ========== Bandpass Filter ==========\n",
        "def bandpass_filter(data, fs, lowcut, highcut, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "# ========== Feature Extraction ==========\n",
        "def get_features(window, fs=1000):\n",
        "    freq_bands = [(5, 15), (20, 25), (75, 115), (125, 160), (160, 175)]\n",
        "    n_channels = window.shape[1]\n",
        "    features = np.zeros((n_channels, 6))\n",
        "    features[:, 0] = np.mean(window, axis=0)\n",
        "    for i, (low, high) in enumerate(freq_bands):\n",
        "        band_filtered = bandpass_filter(window, fs, low, high)\n",
        "        features[:, i + 1] = np.mean(np.abs(band_filtered), axis=0)\n",
        "    return features\n",
        "\n",
        "# ========== Sliding Window ==========\n",
        "def get_windowed_feats(ecog, fs, win_len, overlap):\n",
        "    step = win_len - overlap\n",
        "    num_windows = (ecog.shape[0] - overlap) // step\n",
        "    feats = []\n",
        "    for i in tqdm(range(num_windows), desc=\"Extracting features\"):\n",
        "        start = i * step\n",
        "        end = start + win_len\n",
        "        if end > ecog.shape[0]:\n",
        "            break\n",
        "        window = ecog[start:end, :]\n",
        "        feats.append(get_features(window, fs).flatten())\n",
        "    return np.array(feats)\n",
        "\n",
        "# ========== Create R Matrix ==========\n",
        "def create_R_matrix(features, N_wind):\n",
        "    num_windows, num_feats = features.shape\n",
        "    pad = np.tile(features[0], (N_wind - 1, 1))\n",
        "    padded = np.vstack([pad, features])\n",
        "    R = np.zeros((num_windows, 1 + N_wind * num_feats))\n",
        "    for i in range(num_windows):\n",
        "        context = padded[i:i + N_wind].flatten()\n",
        "        R[i] = np.concatenate(([1], context))\n",
        "    return R\n",
        "\n",
        "# ========== make scorer to maximise pearsonr ==========\n",
        "def mean_pearsonr(y_true, y_pred):\n",
        "    rs = [pearsonr(y_true[:,i], y_pred[:,i])[0]\n",
        "          for i in range(y_true.shape[1])]\n",
        "    return np.mean(rs)\n",
        "\n",
        "pearson_scorer = make_scorer(mean_pearsonr, greater_is_better=True)\n",
        "\n",
        "\n",
        "# ========== Cross-validation with Ridge ==========\n",
        "train_data   = scipy.io.loadmat('raw_training_data.mat')\n",
        "train_ecogs  = train_data['train_ecog']\n",
        "train_gloves = train_data['train_dg']\n",
        "\n",
        "\n",
        "\n",
        "for subj_idx in range(3):\n",
        "    print(f\"\\n=== Subject {subj_idx+1} – Ensemble CV ===\")\n",
        "    ecog  = train_ecogs[subj_idx].item()\n",
        "    glove = train_gloves[subj_idx].item()\n",
        "\n",
        "    feats      = get_windowed_feats(ecog, fs, window_len, overlap)\n",
        "    R          = create_R_matrix(feats, N_wind)\n",
        "    glove_down = signal.decimate(glove,\n",
        "                                 glove.shape[0] // R.shape[0],\n",
        "                                 axis=0, zero_phase=True)[:R.shape[0]]\n",
        "    # smooth labels\n",
        "    for i in range(glove_down.shape[1]):\n",
        "        glove_down[:,i] = gaussian_filter1d(glove_down[:,i], sigma=2.25)\n",
        "\n",
        "    kf    = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    fold_rs = []\n",
        "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_rs  = []\n",
        "    pls_bests = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(R)):\n",
        "        # 1) Split & scale\n",
        "        R_tr, R_te = R[train_idx], R[test_idx]\n",
        "        y_tr, y_te = glove_down[train_idx], glove_down[test_idx]\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        R_tr[:,1:] = scaler.fit_transform(R_tr[:,1:])\n",
        "        R_te[:,1:] = scaler.transform(R_te[:,1:])\n",
        "\n",
        "        # 2) Train each model on the train split\n",
        "        # 2a) Ridge\n",
        "        ridge = Ridge(alpha=final_alphas[subj_idx])\n",
        "        ridge.fit(R_tr[:,1:], y_tr)\n",
        "        pred_ridge = ridge.predict(R_te[:,1:])\n",
        "\n",
        "        # 2b) PLSRegression (tuned by inner CV)\n",
        "        pls_cv = GridSearchCV(\n",
        "            PLSRegression(),\n",
        "            {'n_components': list(range(2, 20, 2))},\n",
        "            scoring=pearson_scorer,\n",
        "            cv=inner_cv,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        pls_cv.fit(R_tr[:,1:], y_tr)\n",
        "\n",
        "        best_n = pls_cv.best_params_['n_components']\n",
        "        pls_bests.append(best_n)\n",
        "        pred_pls = pls_cv.predict(R_te[:,1:])\n",
        "\n",
        "        '''\n",
        "        # 2c) ElasticNet (tuned by inner CV)\n",
        "        enet_cv = GridSearchCV(\n",
        "            ElasticNet(warm_start=True, max_iter=2000, tol=1e-3),\n",
        "            {'alpha': np.logspace(-3,0,7),\n",
        "            'l1_ratio': [0.1, 0.9]},\n",
        "            scoring=pearson_scorer,\n",
        "            cv=inner_cv,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        enet_cv.fit(R_tr[:,1:], y_tr)\n",
        "        pred_enet = enet_cv.predict(R_te[:,1:])\n",
        "        '''\n",
        "\n",
        "        # 3) Average predictions\n",
        "        y_pred = 0.5 * (pred_ridge + pred_pls)\n",
        "\n",
        "\n",
        "        # 4) Post-process & evaluate\n",
        "        y_pred = suppress_low_outliers(y_pred, threshold_multiplier=2)\n",
        "        y_pred = gaussian_smoothing(y_pred, sigma=2.25)\n",
        "        r = [pearsonr(y_te[:,i], y_pred[:,i])[0] for i in range(y_te.shape[1])]\n",
        "        fold_rs.append(np.mean(r))\n",
        "\n",
        "        print(f\"    → Fold {fold_idx+1} r = {fold_rs[-1]:.3f}\")\n",
        "\n",
        "    print(f\"  Fold-wise best PLS components for subject {subj_idx+1}: {pls_bests}\")\n",
        "    print(f\"  Median PLS n_components = {int(np.median(pls_bests))}\")\n",
        "    print(f\" → Avg ensemble r = {np.mean(fold_rs):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpO9wGohYhj0",
        "outputId": "ead79dde-425a-441c-d70d-85c9142aebf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 1 – Ensemble CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:26<00:00, 230.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → Fold 1 r = 0.621\n",
            "    → Fold 2 r = 0.627\n",
            "    → Fold 3 r = 0.609\n",
            "    → Fold 4 r = 0.610\n",
            "    → Fold 5 r = 0.601\n",
            "  Fold-wise best PLS components for subject 1: [14, 14, 18, 14, 14]\n",
            "  Median PLS n_components = 14\n",
            " → Avg ensemble r = 0.614\n",
            "\n",
            "=== Subject 2 – Ensemble CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:23<00:00, 256.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → Fold 1 r = 0.516\n",
            "    → Fold 2 r = 0.504\n",
            "    → Fold 3 r = 0.487\n",
            "    → Fold 4 r = 0.504\n",
            "    → Fold 5 r = 0.510\n",
            "  Fold-wise best PLS components for subject 2: [14, 14, 14, 14, 14]\n",
            "  Median PLS n_components = 14\n",
            " → Avg ensemble r = 0.504\n",
            "\n",
            "=== Subject 3 – Ensemble CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:26<00:00, 229.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    → Fold 1 r = 0.700\n",
            "    → Fold 2 r = 0.704\n",
            "    → Fold 3 r = 0.704\n",
            "    → Fold 4 r = 0.707\n",
            "    → Fold 5 r = 0.699\n",
            "  Fold-wise best PLS components for subject 3: [14, 12, 14, 12, 12]\n",
            "  Median PLS n_components = 12\n",
            " → Avg ensemble r = 0.703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_alphas = [5.609e3, 4.834e3, 3.810e3]\n",
        "final_pls = [14, 14, 12]"
      ],
      "metadata": {
        "id": "NkOT0qAWsojw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick refine PLS"
      ],
      "metadata": {
        "id": "sSOxWHGh4AfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from scipy import signal\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "# ========== Parameters ==========\n",
        "fs = 1000\n",
        "window_len = 100\n",
        "overlap = 50\n",
        "step = window_len - overlap\n",
        "N_wind = 3\n",
        "n_splits = 5\n",
        "final_alphas = [5.609e3, 4.834e3, 3.810e3]\n",
        "median_pls   = [14, 14, 12]  # pre-computed medians per subject\n",
        "\n",
        "# ========== Helper Functions ==========\n",
        "def gaussian_smoothing(pred, sigma=2.25):\n",
        "    pred_smooth = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        pred_smooth[:, i] = gaussian_filter1d(pred[:, i], sigma=sigma)\n",
        "    return pred_smooth\n",
        "\n",
        "def suppress_low_outliers(pred, threshold_multiplier=2):\n",
        "    pred_cleaned = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        col = pred[:, i]\n",
        "        thresh = np.mean(col) - threshold_multiplier * np.std(col)\n",
        "        pred_cleaned[:, i] = np.where(col < thresh, 0, col)\n",
        "    return pred_cleaned\n",
        "\n",
        "def get_features(window, fs=1000):\n",
        "    freq_bands = [(5,15),(20,25),(75,115),(125,160),(160,175)]\n",
        "    n_ch = window.shape[1]\n",
        "    feats = np.zeros((n_ch, 6))\n",
        "    feats[:,0] = np.mean(window, axis=0)\n",
        "    for idx, (low, high) in enumerate(freq_bands):\n",
        "        bf = bandpass_filter(window, fs, low, high)\n",
        "        feats[:,idx+1] = np.mean(np.abs(bf), axis=0)\n",
        "    return feats\n",
        "\n",
        "def bandpass_filter(data, fs, lowcut, highcut, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low, high = lowcut/nyq, highcut/nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "def get_windowed_feats(ecog, fs, win_len, overlap):\n",
        "    step = win_len - overlap\n",
        "    n_win = (ecog.shape[0] - overlap) // step\n",
        "    feats = []\n",
        "    for i in range(n_win):\n",
        "        s, e = i*step, i*step + win_len\n",
        "        if e > ecog.shape[0]: break\n",
        "        feats.append(get_features(ecog[s:e, :], fs).flatten())\n",
        "    return np.array(feats)\n",
        "\n",
        "def create_R_matrix(features, N_wind):\n",
        "    num_windows, num_feats = features.shape\n",
        "    pad = np.tile(features[0], (N_wind-1, 1))\n",
        "    padded = np.vstack([pad, features])\n",
        "    R = np.zeros((num_windows, 1 + N_wind * num_feats))\n",
        "    for i in range(num_windows):\n",
        "        ctx = padded[i:i+N_wind].flatten()\n",
        "        R[i] = np.concatenate(([1], ctx))\n",
        "    return R\n",
        "\n",
        "def mean_pearsonr(y_true, y_pred):\n",
        "    rs = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        r, _ = pearsonr(y_true[:,i], y_pred[:,i])\n",
        "        rs.append(0 if np.isnan(r) else r)\n",
        "    return np.mean(rs)\n",
        "\n",
        "pearson_scorer = make_scorer(mean_pearsonr, greater_is_better=True)\n",
        "\n",
        "# ========== Load Data ==========\n",
        "train_data   = scipy.io.loadmat('raw_training_data.mat')\n",
        "train_ecogs  = train_data['train_ecog']\n",
        "train_gloves = train_data['train_dg']\n",
        "\n",
        "# ========== Cross-Validation with PLS Refinement ==========\n",
        "outer_cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for subj_idx in range(3):\n",
        "    print(f\"\\n=== Subject {subj_idx+1} – CV with ±2 PLS refine ===\")\n",
        "    # build full data for this subject\n",
        "    ecog = train_ecogs[subj_idx].item()\n",
        "    glove = train_gloves[subj_idx].item()\n",
        "\n",
        "    feats = get_windowed_feats(ecog, fs, window_len, overlap)\n",
        "    R = create_R_matrix(feats, N_wind)\n",
        "    y_raw = signal.decimate(glove, glove.shape[0]//R.shape[0], axis=0, zero_phase=True)[:R.shape[0]]\n",
        "    for ch in range(y_raw.shape[1]):\n",
        "        y_raw[:,ch] = gaussian_filter1d(y_raw[:,ch], sigma=2.25)\n",
        "\n",
        "    fold_rs = []\n",
        "    fold_pls_best = []\n",
        "    grid_range = list(range(max(2, median_pls[subj_idx]-2), median_pls[subj_idx]+3))\n",
        "    print(f\"→ refining n_components in {grid_range}\")\n",
        "\n",
        "    for fold, (tr_i, te_i) in enumerate(outer_cv.split(R), 1):\n",
        "        # split and scale\n",
        "        R_tr, R_te = R[tr_i], R[te_i]\n",
        "        y_tr, y_te = y_raw[tr_i], y_raw[te_i]\n",
        "        scaler = StandardScaler().fit(R_tr[:,1:])\n",
        "        X_tr = scaler.transform(R_tr[:,1:])\n",
        "        X_te = scaler.transform(R_te[:,1:])\n",
        "\n",
        "        # Ridge prediction\n",
        "        mr = Ridge(alpha=final_alphas[subj_idx]).fit(X_tr, y_tr)\n",
        "        pr = mr.predict(X_te)\n",
        "\n",
        "        # PLS refinement on this fold\n",
        "        pls_cv = GridSearchCV(\n",
        "            PLSRegression(),\n",
        "            {'n_components': grid_range},\n",
        "            scoring=pearson_scorer,\n",
        "            cv=inner_cv,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        pls_cv.fit(X_tr, y_tr)\n",
        "        best_nc = pls_cv.best_params_['n_components']\n",
        "        fold_pls_best.append(best_nc)\n",
        "        pp = pls_cv.predict(X_te)\n",
        "\n",
        "        # ensemble & post-process\n",
        "        y_pred = 0.5 * (pr + pp)\n",
        "        y_pred = suppress_low_outliers(y_pred, threshold_multiplier=2)\n",
        "        y_pred = gaussian_smoothing(y_pred, sigma=2.25)\n",
        "\n",
        "        # score\n",
        "        r_vals = [pearsonr(y_te[:,i], y_pred[:,i])[0] for i in range(y_te.shape[1])]\n",
        "        fold_r = np.mean(r_vals)\n",
        "        fold_rs.append(fold_r)\n",
        "        print(f\"  Fold {fold} | best PLS={best_nc} | r={fold_r:.3f}\")\n",
        "\n",
        "    print(f\"→ Fold-wise PLS components: {fold_pls_best}\")\n",
        "    print(f\"→ Median refined PLS = {int(np.median(fold_pls_best))}\")\n",
        "    print(f\"→ Avg ensemble r = {np.mean(fold_rs):.3f} ± {np.std(fold_rs):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiquITFh3_WT",
        "outputId": "b8ddbe74-c707-474f-ef46-60db5aba3e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 1 – CV with ±2 PLS refine ===\n",
            "→ refining n_components in [12, 13, 14, 15, 16]\n",
            "  Fold 1 | best PLS=15 | r=0.620\n",
            "  Fold 2 | best PLS=14 | r=0.627\n",
            "  Fold 3 | best PLS=16 | r=0.609\n",
            "  Fold 4 | best PLS=15 | r=0.614\n",
            "  Fold 5 | best PLS=15 | r=0.604\n",
            "→ Fold-wise PLS components: [15, 14, 16, 15, 15]\n",
            "→ Median refined PLS = 15\n",
            "→ Avg ensemble r = 0.615 ± 0.008\n",
            "\n",
            "=== Subject 2 – CV with ±2 PLS refine ===\n",
            "→ refining n_components in [12, 13, 14, 15, 16]\n",
            "  Fold 1 | best PLS=15 | r=0.518\n",
            "  Fold 2 | best PLS=13 | r=0.501\n",
            "  Fold 3 | best PLS=14 | r=0.487\n",
            "  Fold 4 | best PLS=15 | r=0.506\n",
            "  Fold 5 | best PLS=14 | r=0.510\n",
            "→ Fold-wise PLS components: [15, 13, 14, 15, 14]\n",
            "→ Median refined PLS = 14\n",
            "→ Avg ensemble r = 0.504 ± 0.010\n",
            "\n",
            "=== Subject 3 – CV with ±2 PLS refine ===\n",
            "→ refining n_components in [10, 11, 12, 13, 14]\n",
            "  Fold 1 | best PLS=13 | r=0.699\n",
            "  Fold 2 | best PLS=13 | r=0.706\n",
            "  Fold 3 | best PLS=13 | r=0.704\n",
            "  Fold 4 | best PLS=13 | r=0.712\n",
            "  Fold 5 | best PLS=13 | r=0.701\n",
            "→ Fold-wise PLS components: [13, 13, 13, 13, 13]\n",
            "→ Median refined PLS = 13\n",
            "→ Avg ensemble r = 0.704 ± 0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "refined_pls = [15, 14, 13]"
      ],
      "metadata": {
        "id": "scVQqJeY7pld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tune weight of each model"
      ],
      "metadata": {
        "id": "qiU_mudXickG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from scipy import signal\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "# ========== Parameters ==========\n",
        "fs = 1000\n",
        "window_len = 100\n",
        "overlap = 50\n",
        "step = window_len - overlap\n",
        "N_wind = 3\n",
        "n_splits = 5\n",
        "# Tuned hyperparameters per subject\n",
        "final_alphas = [5.609e3, 4.834e3, 3.810e3]\n",
        "refined_pls   = [15, 14, 13]\n",
        "\n",
        "# ========== Helper Functions ==========\n",
        "def gaussian_smoothing(pred, sigma=2.25):\n",
        "    out = pred.copy()\n",
        "    for i in range(out.shape[1]):\n",
        "        out[:, i] = gaussian_filter1d(out[:, i], sigma)\n",
        "    return out\n",
        "\n",
        "def suppress_low_outliers(pred, threshold_multiplier=2):\n",
        "    out = pred.copy()\n",
        "    for i in range(out.shape[1]):\n",
        "        col = out[:, i]\n",
        "        thr = col.mean() - threshold_multiplier * col.std()\n",
        "        out[:, i] = np.where(col < thr, 0, col)\n",
        "    return out\n",
        "\n",
        "def bandpass_filter(data, fs, lowcut, highcut, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low, high = lowcut/nyq, highcut/nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "def get_features(window, fs=1000):\n",
        "    bands = [(5,15),(20,25),(75,115),(125,160),(160,175)]\n",
        "    n_ch  = window.shape[1]\n",
        "    feats = np.zeros((n_ch, 6))\n",
        "    feats[:,0] = window.mean(axis=0)\n",
        "    for idx, (low, high) in enumerate(bands):\n",
        "        bf = bandpass_filter(window, fs, low, high)\n",
        "        feats[:,idx+1] = np.abs(bf).mean(axis=0)\n",
        "    return feats\n",
        "\n",
        "def get_windowed_feats(ecog, fs, win_len, overlap):\n",
        "    step = win_len - overlap\n",
        "    n_win = (ecog.shape[0] - overlap)//step\n",
        "    feats = []\n",
        "    for i in tqdm(range(n_win), desc=\"Extracting features\"):\n",
        "        s, e = i*step, i*step + win_len\n",
        "        if e > ecog.shape[0]: break\n",
        "        feats.append(get_features(ecog[s:e,:], fs).flatten())\n",
        "    return np.array(feats)\n",
        "\n",
        "def create_R_matrix(features, N_wind):\n",
        "    n_w, n_f = features.shape\n",
        "    pad   = np.tile(features[0], (N_wind-1,1))\n",
        "    concat= np.vstack([pad, features])\n",
        "    R     = np.zeros((n_w, 1 + N_wind*n_f))\n",
        "    for i in range(n_w):\n",
        "        ctx = concat[i:i+N_wind].ravel()\n",
        "        R[i] = np.concatenate(([1], ctx))\n",
        "    return R\n",
        "\n",
        "def mean_pearsonr(y_true, y_pred):\n",
        "    rs = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        r, _ = pearsonr(y_true[:,i], y_pred[:,i])\n",
        "        rs.append(0 if np.isnan(r) else r)\n",
        "    return np.mean(rs)\n",
        "\n",
        "pearson_scorer = make_scorer(mean_pearsonr, greater_is_better=True)\n",
        "\n",
        "def tune_weight(X, y, alpha, n_comp, n_splits=5):\n",
        "    \"\"\"\n",
        "    Returns (best_w, best_r) by mixing post-processed Ridge and PLS on OOF preds.\n",
        "    \"\"\"\n",
        "    oof_pr = np.zeros_like(y)\n",
        "    oof_pp = np.zeros_like(y)\n",
        "    oof_y  = np.zeros_like(y)\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
        "    for tr_i, te_i in kf.split(X):\n",
        "        # train models\n",
        "        mr = Ridge(alpha=alpha).fit(X[tr_i], y[tr_i])\n",
        "        pl = PLSRegression(n_components=n_comp).fit(X[tr_i], y[tr_i])\n",
        "        # raw preds\n",
        "        pr = mr.predict(X[te_i])\n",
        "        pp = pl.predict(X[te_i])\n",
        "        # post-process\n",
        "        pr = suppress_low_outliers(pr, threshold_multiplier=2)\n",
        "        pr = gaussian_smoothing(pr, sigma=2.25)\n",
        "        pp = suppress_low_outliers(pp, threshold_multiplier=2)\n",
        "        pp = gaussian_smoothing(pp, sigma=2.25)\n",
        "        # store OOF\n",
        "        oof_pr[te_i] = pr\n",
        "        oof_pp[te_i] = pp\n",
        "        oof_y [te_i] = y[te_i]\n",
        "    best_w, best_r = 0.0, -np.inf\n",
        "    for w in np.linspace(0,1,21):\n",
        "        pred = w*oof_pr + (1-w)*oof_pp\n",
        "        rs   = [pearsonr(oof_y[:,i], pred[:,i])[0] for i in range(y.shape[1])]\n",
        "        r    = np.mean(rs)\n",
        "        if r > best_r:\n",
        "            best_r, best_w = r, w\n",
        "    return best_w, best_r\n",
        "\n",
        "# ========== Load Data ==========\n",
        "train = scipy.io.loadmat('raw_training_data.mat')\n",
        "train_ecogs  = train['train_ecog']\n",
        "train_gloves = train['train_dg']\n",
        "\n",
        "# ========== CV with PLS Refinement & Weight Tuning ==========\n",
        "outer_cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for subj_idx in range(3):\n",
        "    print(f\"\\n=== Subject {subj_idx+1} – CV w/ PLS refine & w-tuning ===\")\n",
        "    # full-train features & labels\n",
        "    E = train_ecogs[subj_idx].item()\n",
        "    G = train_gloves[subj_idx].item()\n",
        "    feats = get_windowed_feats(E, fs, window_len, overlap)\n",
        "    R     = create_R_matrix(feats, N_wind)\n",
        "    y_raw = signal.decimate(G, G.shape[0]//R.shape[0], axis=0, zero_phase=True)[:R.shape[0]]\n",
        "    for ch in range(y_raw.shape[1]):\n",
        "        y_raw[:,ch] = gaussian_filter1d(y_raw[:,ch], sigma=2.25)\n",
        "    # scale full data\n",
        "    scaler = StandardScaler().fit(R[:,1:])\n",
        "    X_full = scaler.transform(R[:,1:])\n",
        "    y_full = y_raw\n",
        "    # tune ensemble weight\n",
        "    w_opt, r_opt = tune_weight(\n",
        "        X_full, y_full,\n",
        "        alpha = final_alphas[subj_idx],\n",
        "        n_comp= refined_pls[subj_idx],\n",
        "        n_splits=5\n",
        "    )\n",
        "    print(f\"→ Optimal w = {w_opt:.2f}, global OOF r = {r_opt:.3f}\")\n",
        "    # outer CV folds\n",
        "    fold_rs, fold_pls_best = [], []\n",
        "    grid_range = list(range(max(2, refined_pls[subj_idx]-2),\n",
        "                             refined_pls[subj_idx]+3))\n",
        "    print(f\"→ refining PLS comps in {grid_range}\")\n",
        "    for fold, (tr_i, te_i) in enumerate(outer_cv.split(R), 1):\n",
        "        R_tr, R_te = R[tr_i], R[te_i]\n",
        "        y_tr, y_te = y_full[tr_i], y_full[te_i]\n",
        "        X_tr = scaler.transform(R_tr[:,1:])\n",
        "        X_te = scaler.transform(R_te[:,1:])\n",
        "        pr = Ridge(alpha=final_alphas[subj_idx]).fit(X_tr, y_tr).predict(X_te)\n",
        "        pls_cv = GridSearchCV(\n",
        "            PLSRegression(), {'n_components': grid_range},\n",
        "            scoring=pearson_scorer, cv=inner_cv, n_jobs=-1\n",
        "        )\n",
        "        pls_cv.fit(X_tr, y_tr)\n",
        "        best_nc = pls_cv.best_params_['n_components']\n",
        "        fold_pls_best.append(best_nc)\n",
        "        pp = pls_cv.predict(X_te)\n",
        "        y_pred = w_opt*pr + (1-w_opt)*pp\n",
        "        y_pred = suppress_low_outliers(y_pred, 2)\n",
        "        y_pred = gaussian_smoothing(y_pred, sigma=2.25)\n",
        "        rs = [pearsonr(y_te[:,i], y_pred[:,i])[0] for i in range(y_te.shape[1])]\n",
        "        fold_r = np.mean(rs)\n",
        "        fold_rs.append(fold_r)\n",
        "        print(f\"  Fold {fold} | best PLS={best_nc} | r={fold_r:.3f}\")\n",
        "    print(f\"→ Fold-wise PLS picks: {fold_pls_best}\")\n",
        "    print(f\"→ Median PLS = {int(np.median(fold_pls_best))}\")\n",
        "    print(f\"→ Avg  r = {np.mean(fold_rs):.3f} ± {np.std(fold_rs):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Df4E_VnifGy",
        "outputId": "1291e1e0-6e08-4529-c01c-8d55701fc659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 1 – CV w/ PLS refine & w-tuning ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:22<00:00, 262.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Optimal w = 0.50, global OOF r = 0.608\n",
            "→ refining PLS comps in [13, 14, 15, 16, 17]\n",
            "  Fold 1 | best PLS=15 | r=0.620\n",
            "  Fold 2 | best PLS=14 | r=0.627\n",
            "  Fold 3 | best PLS=17 | r=0.610\n",
            "  Fold 4 | best PLS=17 | r=0.610\n",
            "  Fold 5 | best PLS=15 | r=0.604\n",
            "→ Fold-wise PLS picks: [15, 14, 17, 17, 15]\n",
            "→ Median PLS = 15\n",
            "→ Avg  r = 0.614 ± 0.008\n",
            "\n",
            "=== Subject 2 – CV w/ PLS refine & w-tuning ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:21<00:00, 277.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Optimal w = 0.35, global OOF r = 0.502\n",
            "→ refining PLS comps in [12, 13, 14, 15, 16]\n",
            "  Fold 1 | best PLS=15 | r=0.517\n",
            "  Fold 2 | best PLS=13 | r=0.498\n",
            "  Fold 3 | best PLS=14 | r=0.487\n",
            "  Fold 4 | best PLS=15 | r=0.507\n",
            "  Fold 5 | best PLS=14 | r=0.510\n",
            "→ Fold-wise PLS picks: [15, 13, 14, 15, 14]\n",
            "→ Median PLS = 14\n",
            "→ Avg  r = 0.504 ± 0.010\n",
            "\n",
            "=== Subject 3 – CV w/ PLS refine & w-tuning ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:23<00:00, 259.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Optimal w = 0.65, global OOF r = 0.701\n",
            "→ refining PLS comps in [11, 12, 13, 14, 15]\n",
            "  Fold 1 | best PLS=13 | r=0.699\n",
            "  Fold 2 | best PLS=13 | r=0.707\n",
            "  Fold 3 | best PLS=13 | r=0.704\n",
            "  Fold 4 | best PLS=13 | r=0.712\n",
            "  Fold 5 | best PLS=13 | r=0.701\n",
            "→ Fold-wise PLS picks: [13, 13, 13, 13, 13]\n",
            "→ Median PLS = 13\n",
            "→ Avg  r = 0.705 ± 0.005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from scipy import signal\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# ========== Parameters ==========\n",
        "fs = 1000\n",
        "window_len = 100\n",
        "overlap = 50\n",
        "step = window_len - overlap\n",
        "N_wind = 3\n",
        "\n",
        "# ========== Gaussian Smoothing ==========\n",
        "def gaussian_smoothing(pred, sigma=2.25):\n",
        "    pred_smooth = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        pred_smooth[:, i] = gaussian_filter1d(pred[:, i], sigma=sigma)\n",
        "    return pred_smooth\n",
        "\n",
        "# ========== Outlier Suppression ==========\n",
        "def suppress_low_outliers(pred, threshold_multiplier=2):\n",
        "    pred_cleaned = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        col = pred[:, i]\n",
        "        mean = np.mean(col)\n",
        "        std = np.std(col)\n",
        "        thresh = mean - threshold_multiplier * std\n",
        "        pred_cleaned[:, i] = np.where(col < thresh, 0, col)\n",
        "    return pred_cleaned\n",
        "\n",
        "# ========== Bandpass Filter ==========\n",
        "def bandpass_filter(data, fs, lowcut, highcut, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low, high = lowcut/nyq, highcut/nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "# ========== Feature Extraction ==========\n",
        "def get_features(window, fs=1000):\n",
        "    freq_bands = [(5,15),(20,25),(75,115),(125,160),(160,175)]\n",
        "    n_ch = window.shape[1]\n",
        "    feats = np.zeros((n_ch, 6))\n",
        "    feats[:,0] = np.mean(window,axis=0)\n",
        "    for i,(low,high) in enumerate(freq_bands):\n",
        "        bf = bandpass_filter(window, fs, low, high)\n",
        "        feats[:,i+1] = np.mean(np.abs(bf),axis=0)\n",
        "    return feats\n",
        "\n",
        "# ========== Sliding Window ==========\n",
        "def get_windowed_feats(ecog, fs, win_len, overlap):\n",
        "    step = win_len - overlap\n",
        "    n_win = (ecog.shape[0] - overlap)//step\n",
        "    feats = []\n",
        "    for i in tqdm(range(n_win), desc=\"Extracting features\"):\n",
        "        s, e = i*step, i*step+win_len\n",
        "        if e > ecog.shape[0]: break\n",
        "        feats.append(get_features(ecog[s:e,:], fs).flatten())\n",
        "    return np.array(feats)\n",
        "\n",
        "# ========== Create R Matrix ==========\n",
        "def create_R_matrix(features, N_wind):\n",
        "    n_w, n_f = features.shape\n",
        "    pad = np.tile(features[0], (N_wind-1,1))\n",
        "    concat = np.vstack([pad, features])\n",
        "    R = np.zeros((n_w, 1 + N_wind*n_f))\n",
        "    for i in range(n_w):\n",
        "        ctx = concat[i:i+N_wind].flatten()\n",
        "        R[i] = np.concatenate(([1], ctx))\n",
        "    return R\n",
        "\n",
        "# ========== Scorer for Pearson-r ==========\n",
        "def mean_pearsonr(y_true, y_pred):\n",
        "    rs = [pearsonr(y_true[:,i], y_pred[:,i])[0]\n",
        "          for i in range(y_true.shape[1])]\n",
        "    return np.mean(rs)\n",
        "\n",
        "pearson_scorer = make_scorer(mean_pearsonr, greater_is_better=True)\n",
        "\n",
        "# ==========Tuned parameters ==========\n",
        "final_alphas = [5609, 4834, 3810]\n",
        "refined_pls  = [15  ,   14,   13]\n",
        "weights      = [0.50, 0.35, 0.65]\n",
        "\n",
        "# ========== Predict on Leaderboard ==========\n",
        "train_data        = scipy.io.loadmat('raw_training_data.mat')\n",
        "test_data         = scipy.io.loadmat('leaderboard_data.mat')\n",
        "train_ecogs       = train_data['train_ecog']\n",
        "train_gloves      = train_data['train_dg']\n",
        "leaderboard_ecogs = test_data['leaderboard_ecog']\n",
        "predicted_dg      = np.empty((3,1),dtype=object)\n",
        "\n",
        "for subj_idx in range(3):\n",
        "    print(f\"\\n=== Subject {subj_idx+1} ===\")\n",
        "    # load\n",
        "    ecog_tr = train_ecogs[subj_idx].item()\n",
        "    glove_tr= train_gloves[subj_idx].item()\n",
        "    ecog_te = leaderboard_ecogs[subj_idx].item()\n",
        "\n",
        "    # train features & labels\n",
        "    feats_tr  = get_windowed_feats(ecog_tr, fs, window_len, overlap)\n",
        "    R_tr      = create_R_matrix(feats_tr, N_wind)\n",
        "    glove_ds  = signal.decimate(glove_tr,\n",
        "                   glove_tr.shape[0]//R_tr.shape[0], axis=0,\n",
        "                   zero_phase=True)[:R_tr.shape[0]]\n",
        "    # smooth labels\n",
        "    for i in range(glove_ds.shape[1]):\n",
        "        glove_ds[:,i] = gaussian_filter1d(glove_ds[:,i], sigma=2.25)\n",
        "\n",
        "    # scale\n",
        "    scaler = StandardScaler()\n",
        "    R_tr[:,1:] = scaler.fit_transform(R_tr[:,1:])\n",
        "\n",
        "    #Fit\n",
        "    #Ridge\n",
        "    modridge = Ridge(alpha=final_alphas[subj_idx]).fit(R_tr[:,1:], glove_ds)\n",
        "\n",
        "    #PLS\n",
        "    pls = PLSRegression(n_components=refined_pls[subj_idx]).fit(R_tr[:,1:], glove_ds)\n",
        "\n",
        "    # test features & predict\n",
        "    feats_te   = get_windowed_feats(ecog_te, fs, window_len, overlap)\n",
        "    R_te       = create_R_matrix(feats_te, N_wind)\n",
        "    R_te[:,1:] = scaler.transform(R_te[:,1:])\n",
        "    pr = modridge.predict(R_te[:,1:])\n",
        "    pp = pls.predict(R_te[:,1:])\n",
        "    w  = weights[subj_idx]\n",
        "    pred = w*pr + (1-w)*pp\n",
        "\n",
        "    # post-process\n",
        "    pred = suppress_low_outliers(pred, threshold_multiplier=2)\n",
        "    pred = gaussian_smoothing(pred, sigma=2.25)\n",
        "\n",
        "    # upsample back to 1kHz\n",
        "    x_old = np.arange(pred.shape[0]) * step\n",
        "    x_new = np.arange(ecog_te.shape[0])\n",
        "    interp = np.zeros((len(x_new), pred.shape[1]))\n",
        "    for i in range(pred.shape[1]):\n",
        "        cs = CubicSpline(x_old, pred[:,i], bc_type='natural')\n",
        "        interp[:,i] = cs(x_new)\n",
        "\n",
        "    predicted_dg[subj_idx,0] = interp\n",
        "\n",
        "# save\n",
        "scipy.io.savemat('predicted_submission.mat',\n",
        "                 {'predicted_dg': predicted_dg})\n",
        "print(\"\\n✅ Predictions saved to 'predicted_submission.mat'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6BNso5ds5",
        "outputId": "8d034c10-12e8-41ca-f5b1-869dc4bf2b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:25<00:00, 232.08it/s]\n",
            "Extracting features: 100%|██████████| 2949/2949 [00:12<00:00, 236.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:23<00:00, 256.30it/s]\n",
            "Extracting features: 100%|██████████| 2949/2949 [00:11<00:00, 255.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 3 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:26<00:00, 227.87it/s]\n",
            "Extracting features: 100%|██████████| 2949/2949 [00:12<00:00, 234.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Predictions saved to 'predicted_submission.mat'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d-JGAZpW2sEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final submission (tuned Ridge)\n",
        "Train and save model to use on final dataset"
      ],
      "metadata": {
        "id": "eVi1NRuHePU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "from scipy import signal\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "\n",
        "# ========== Parameters ==========\n",
        "fs = 1000\n",
        "window_len = 100\n",
        "overlap = 50\n",
        "step = window_len - overlap\n",
        "N_wind = 3\n",
        "\n",
        "# ========== Gaussian Smoothing ==========\n",
        "def gaussian_smoothing(pred, sigma=2.25):\n",
        "    pred_smooth = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        pred_smooth[:, i] = gaussian_filter1d(pred[:, i], sigma=sigma)\n",
        "    return pred_smooth\n",
        "\n",
        "# ========== Outlier Suppression ==========\n",
        "def suppress_low_outliers(pred, threshold_multiplier=2):\n",
        "    pred_cleaned = pred.copy()\n",
        "    for i in range(pred.shape[1]):\n",
        "        col = pred[:, i]\n",
        "        mean = np.mean(col)\n",
        "        std = np.std(col)\n",
        "        thresh = mean - threshold_multiplier * std\n",
        "        pred_cleaned[:, i] = np.where(col < thresh, 0, col)\n",
        "    return pred_cleaned\n",
        "\n",
        "# ========== Bandpass Filter ==========\n",
        "def bandpass_filter(data, fs, lowcut, highcut, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low, high = lowcut/nyq, highcut/nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return signal.filtfilt(b, a, data, axis=0)\n",
        "\n",
        "# ========== Feature Extraction ==========\n",
        "def get_features(window, fs=1000):\n",
        "    freq_bands = [(5,15),(20,25),(75,115),(125,160),(160,175)]\n",
        "    n_ch = window.shape[1]\n",
        "    feats = np.zeros((n_ch, 6))\n",
        "    feats[:,0] = np.mean(window,axis=0)\n",
        "    for i,(low,high) in enumerate(freq_bands):\n",
        "        bf = bandpass_filter(window, fs, low, high)\n",
        "        feats[:,i+1] = np.mean(np.abs(bf),axis=0)\n",
        "    return feats\n",
        "\n",
        "# ========== Sliding Window ==========\n",
        "def get_windowed_feats(ecog, fs, win_len, overlap):\n",
        "    step = win_len - overlap\n",
        "    n_win = (ecog.shape[0] - overlap)//step\n",
        "    feats = []\n",
        "    for i in tqdm(range(n_win), desc=\"Extracting features\"):\n",
        "        s, e = i*step, i*step+win_len\n",
        "        if e > ecog.shape[0]: break\n",
        "        feats.append(get_features(ecog[s:e,:], fs).flatten())\n",
        "    return np.array(feats)\n",
        "\n",
        "# ========== Create R Matrix ==========\n",
        "def create_R_matrix(features, N_wind):\n",
        "    n_w, n_f = features.shape\n",
        "    pad = np.tile(features[0], (N_wind-1,1))\n",
        "    concat = np.vstack([pad, features])\n",
        "    R = np.zeros((n_w, 1 + N_wind*n_f))\n",
        "    for i in range(n_w):\n",
        "        ctx = concat[i:i+N_wind].flatten()\n",
        "        R[i] = np.concatenate(([1], ctx))\n",
        "    return R\n",
        "\n",
        "\n",
        "# ==========Tuned parameters ==========\n",
        "final_alphas = [5609, 4834, 3810]\n",
        "\n",
        "# ==========Tuned parameters ==========\n",
        "savemodels = []\n",
        "\n",
        "\n",
        "# ========== Predict on Leaderboard ==========\n",
        "train_data        = scipy.io.loadmat('raw_training_data.mat')\n",
        "train_ecogs       = train_data['train_ecog']\n",
        "train_gloves      = train_data['train_dg']\n",
        "\n",
        "for subj_idx in range(3):\n",
        "    print(f\"\\n=== Subject {subj_idx+1} ===\")\n",
        "    # load\n",
        "    ecog_tr = train_ecogs[subj_idx].item()\n",
        "    glove_tr= train_gloves[subj_idx].item()\n",
        "\n",
        "    # train features & labels\n",
        "    feats_tr  = get_windowed_feats(ecog_tr, fs, window_len, overlap)\n",
        "    R_tr      = create_R_matrix(feats_tr, N_wind)\n",
        "    glove_ds  = signal.decimate(glove_tr,\n",
        "                   glove_tr.shape[0]//R_tr.shape[0], axis=0,\n",
        "                   zero_phase=True)[:R_tr.shape[0]]\n",
        "    # smooth labels\n",
        "    for i in range(glove_ds.shape[1]):\n",
        "        glove_ds[:,i] = gaussian_filter1d(glove_ds[:,i], sigma=2.25)\n",
        "\n",
        "\n",
        "\n",
        "    # Build and fit pipeline: scaler + Ridge\n",
        "    model = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        Ridge(alpha=final_alphas[subj_idx])\n",
        "    )\n",
        "    model.fit(R_tr[:,1:], glove_ds)\n",
        "    savemodels.append(model)\n",
        "\n",
        "\n",
        "#dump into joblib\n",
        "joblib.dump(savemodels, 'ridge_models.joblib', compress=3)\n",
        "print(\"Saved ridgemodels to 'ridge_models.joblib'\")\n"
      ],
      "metadata": {
        "id": "2ok-l0Nq3o2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31133638-bac4-4221-84b3-592a05eac6aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:26<00:00, 227.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:23<00:00, 252.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Subject 3 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 5999/5999 [00:26<00:00, 226.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved ridgemodels to 'ridge_models.joblib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "savemodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6d0Ylgwo9Wi",
        "outputId": "d46e921a-2aeb-4f88-94ac-27dbf1f30504"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Pipeline(steps=[('standardscaler', StandardScaler()),\n",
              "                 ('ridge', Ridge(alpha=5609))]),\n",
              " Pipeline(steps=[('standardscaler', StandardScaler()),\n",
              "                 ('ridge', Ridge(alpha=4834))]),\n",
              " Pipeline(steps=[('standardscaler', StandardScaler()),\n",
              "                 ('ridge', Ridge(alpha=3810))])]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create test dataset to debug code with\n",
        "import scipy.io\n",
        "\n",
        "# 1. Load the .mat\n",
        "data = scipy.io.loadmat('leaderboard_data.mat')\n",
        "\n",
        "print(data.keys())\n",
        "\n",
        "# 2. Change keys to match document\n",
        "data['truetest_data'] = data['leaderboard_ecog']\n",
        "del data['leaderboard_ecog']\n",
        "\n",
        "# 3. Save as test dataset\n",
        "scipy.io.savemat('truetest_data.mat', data)"
      ],
      "metadata": {
        "id": "di06vPO9swyk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}